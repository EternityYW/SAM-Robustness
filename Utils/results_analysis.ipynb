{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score\n",
    "\n",
    "def compute_iou(ground_truth_binary, predicted_binary, epsilon=1e-7):\n",
    "    intersection = np.logical_and(ground_truth_binary, predicted_binary).sum()\n",
    "    union = np.logical_or(ground_truth_binary, predicted_binary).sum()\n",
    "    iou = intersection / (union + epsilon)\n",
    "    return iou\n",
    "\n",
    "def compute_f1(ground_truth_binary, predicted_binary, epsilon=1e-7):\n",
    "    true_positives = np.logical_and(ground_truth_binary, predicted_binary).sum()\n",
    "    false_positives = np.logical_and(np.logical_not(ground_truth_binary), predicted_binary).sum()\n",
    "    false_negatives = np.logical_and(ground_truth_binary, np.logical_not(predicted_binary)).sum()\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + epsilon)\n",
    "    recall = true_positives / (true_positives + false_negatives + epsilon)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['forest', 'fish', 'fire', 'ultrasound', 'radiology', 'water', 'dancing', 'road', 'crack']\n",
    "data_label_prefixes = ['f', 'fish', 'fire', 'm', 'r', 'w', 'd', 'road', 'c']\n",
    "perturbations = ['raw', 'bright', 'chromatic_aberration', 'compressed', 'contrast', 'defocus_blur', 'motion_blur', 'gaussian_noise', 'salt_pepper_noise', 'elastic_transform', 'fog', 'gaussian_noise', 'radial_distortion', 'saturation', 'shot_noise', 'snow']\n",
    "prompt_types = ['point', 'box', 'point_box']\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset, data_label_prefix in zip(datasets, data_label_prefixes):\n",
    "    for perturbation in perturbations:\n",
    "        for prompt_type in prompt_types:\n",
    "            iou_sum = 0\n",
    "            f1_sum = 0\n",
    "            recall_sum = 0\n",
    "            precision_sum = 0\n",
    "            count = 0\n",
    "            for i in range(1, 11):\n",
    "                data_label = f'{data_label_prefix}{i}'\n",
    "                gt_path = os.path.join(str(dataset), f'{data_label}_raw_gt.png' if dataset in ('ultrasound', 'radiology', 'fish', 'dancing', 'road') else f'{data_label}_raw_gt.jpg')\n",
    "                pred_path = os.path.join(dataset, f'{data_label}_{perturbation}_{prompt_type}_mask.npy')\n",
    "                gt = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "                pred = np.load(pred_path)\n",
    "                \n",
    "                if data_label_prefix == \"r\":\n",
    "                    # Resize the predicted masks to match the size of the ground truth mask\n",
    "                    pred = cv2.resize(pred[0], (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                    \n",
    "                ground_truth_binary = gt == 255\n",
    "                predicted_binary = pred == 255\n",
    "                iou = compute_iou(ground_truth_binary, predicted_binary)\n",
    "                precision, recall, f1_val = compute_f1(ground_truth_binary, predicted_binary)\n",
    "                iou_sum += iou\n",
    "                f1_sum += f1_val\n",
    "                recall_sum += recall\n",
    "                precision_sum += precision\n",
    "                count += 1\n",
    "            if count > 0:\n",
    "                results.append((dataset, perturbation, prompt_type, iou_sum / count, f1_sum / count, recall_sum / count, precision_sum / count))\n",
    "\n",
    "# Print the results in a tabular format\n",
    "print(\"Dataset\\tPerturbation\\tPrompt Type\\tIoU\\tF1\\tRecall\\tPrecision\")\n",
    "for res in results:\n",
    "    print(f\"{res[0]}\\t{res[1]}\\t{res[2]}\\t{res[3]:.4f}\\t{res[4]:.4f}\\t{res[5]:.4f}\\t{res[6]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average performance metrics across all datasets\n",
    "average_results = {}\n",
    "for res in results:\n",
    "    key = (res[1], res[2])\n",
    "    if key not in average_results:\n",
    "        average_results[key] = [0, 0, 0, 0, 0]\n",
    "    average_results[key][0] += res[3]\n",
    "    average_results[key][1] += res[4]\n",
    "    average_results[key][2] += res[5]\n",
    "    average_results[key][3] += res[6]\n",
    "    average_results[key][4] += 1\n",
    "\n",
    "for key in average_results:\n",
    "    average_results[key] = [x / average_results[key][4] for x in average_results[key][:4]]\n",
    "\n",
    "# Print the average results in a tabular format\n",
    "print(\"Perturbation\\tPrompt Type\\tAverage IoU\\tAverage F1\\tAverage Recall\\tAverage Precision\")\n",
    "for key, values in average_results.items():\n",
    "    print(f\"{key[0]}\\t{key[1]}\\t{values[0]:.4f}\\t{values[1]:.4f}\\t{values[2]:.4f}\\t{values[3]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "viridis_cmap = cm.get_cmap('viridis')\n",
    "norm = plt.Normalize(0, len(prompt_types))\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "# Prepare the data for plotting\n",
    "plot_data = {}\n",
    "for key, values in average_results.items():\n",
    "    if key[0] not in plot_data:\n",
    "        plot_data[key[0]] = {}\n",
    "    plot_data[key[0]][key[1]] = values\n",
    "\n",
    "perturbation_categories = {\n",
    "    \"Raw\": [\"raw\"],\n",
    "    \"Noise\": [\"gaussian_noise\", \"shot_noise\", \"salt_pepper_noise\"],\n",
    "    \"Blur\": [\"gaussian_blur\", \"motion_blur\", \"defocus_blur\"],\n",
    "    \"OG\": [\"chromatic_aberration\", \"elastic_transform\", \"radial_distortion\"],\n",
    "    \"IC\": [\"brightness\", \"saturation\", \"contrast\"],\n",
    "    \"ENV\": [\"snow\", \"fog\"],\n",
    "    \"CMP\": [\"compressed\"],\n",
    "}\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "metrics = ['IoU', 'F1', 'Recall', 'Precision']\n",
    "\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        metric_idx = row * 2 + col\n",
    "        metric = metrics[metric_idx]\n",
    "        ax = axes[row, col]\n",
    "        x = np.arange(len(perturbation_categories))\n",
    "        width = 0.2\n",
    "\n",
    "        for idx, prompt_type in enumerate(prompt_types):\n",
    "            y = [np.mean([plot_data[p][prompt_type][metric_idx] for p in perturbation_categories[category] if p in plot_data]) for category in perturbation_categories]\n",
    "            color = viridis_cmap(norm(idx))\n",
    "            ax.bar(x + (idx - 1) * width, y, width, label=prompt_type, color=color)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(perturbation_categories.keys(), rotation=0, fontsize=11)\n",
    "        ax.set_ylabel(f'Average {metric}', fontsize=12)\n",
    "        ax.set_title(f'Prompting Effect on {metric}')\n",
    "legend = fig.legend(['Point', 'Box', 'Combination of Point and Box'], loc='upper center', bbox_to_anchor=(0.5, -0.01), ncol=3, fontsize=11)\n",
    "fig.add_artist(legend)\n",
    "plt.subplots_adjust(bottom=0.15, hspace=0.4)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"prompting_effect_metrics.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare the data for the table\n",
    "table_data = {\n",
    "    \"Prompting Technique\": [],\n",
    "    \"Metric\": [],\n",
    "    \"Raw\": [],\n",
    "    \"Perturbed\": [],\n",
    "    \"Change (%)\": []\n",
    "}\n",
    "\n",
    "for prompt_type in prompt_types:\n",
    "    for metric_idx, metric in enumerate(metrics):\n",
    "        table_data[\"Prompting Technique\"].append(prompt_type)\n",
    "        table_data[\"Metric\"].append(metric)\n",
    "        raw_average = plot_data[\"raw\"][prompt_type][metric_idx]\n",
    "        table_data[\"Raw\"].append(raw_average)\n",
    "\n",
    "        perturbed_average = []\n",
    "        for category in perturbation_categories:\n",
    "            if category == \"Raw\":\n",
    "                continue\n",
    "            perturbed_average.extend([plot_data[p][prompt_type][metric_idx] for p in perturbation_categories[category] if p in plot_data])\n",
    "        perturbed_mean = np.mean(perturbed_average)\n",
    "        table_data[\"Perturbed\"].append(perturbed_mean)\n",
    "\n",
    "        # Calculate the percentage change\n",
    "        percentage_change = ((perturbed_mean - raw_average) / raw_average) * 100\n",
    "        table_data[\"Change (%)\"].append(percentage_change)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "performance_table = pd.DataFrame(table_data)\n",
    "\n",
    "# Display the table\n",
    "print(performance_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the numbers in the table\n",
    "performance_table_rounded = performance_table.round({'Raw': 3, 'Perturbed': 3, 'Change (%)': 2})\n",
    "\n",
    "# Convert the DataFrame to a LaTeX-formatted table\n",
    "latex_table = performance_table_rounded.to_latex(index=False)\n",
    "\n",
    "# Print the LaTeX-formatted table\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "datasets = ['forest', 'fish', 'fire', 'ultrasound', 'radiology', 'water', 'dancing', 'road', 'crack']\n",
    "data_label_prefixes = ['f', 'fish', 'fire', 'm', 'r', 'w', 'd', 'road', 'c']\n",
    "perturbations = ['raw', 'bright', 'chromatic_aberration', 'compressed', 'contrast', 'defocus_blur', 'motion_blur', 'gaussian_noise', 'salt_pepper_noise', 'elastic_transform', 'fog', 'gaussian_blur', 'radial_distortion', 'saturation', 'shot_noise', 'snow']\n",
    "prompt_types = ['point', 'box', 'point_box']\n",
    "top_k = 4\n",
    "\n",
    "results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for dataset, data_label_prefix in zip(datasets, data_label_prefixes):\n",
    "    for prompt_type in prompt_types:\n",
    "        for perturbation in perturbations:\n",
    "            iou_sum = 0\n",
    "            f1_sum = 0\n",
    "            recall_sum = 0\n",
    "            precision_sum = 0\n",
    "            count = 0\n",
    "            for i in range(1, 11):\n",
    "                data_label = f'{data_label_prefix}{i}'\n",
    "                gt_path = os.path.join(str(dataset), f'{data_label}_raw_gt.png' if dataset in ('ultrasound', 'radiology', 'fish', 'dancing', 'road') else f'{data_label}_raw_gt.jpg')\n",
    "                pred_path = os.path.join(dataset, f'{data_label}_{perturbation}_{prompt_type}_mask.npy')\n",
    "                gt = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "                pred = np.load(pred_path)\n",
    "\n",
    "                if data_label_prefix == \"r\":\n",
    "                    # Resize the predicted masks to match the size of the ground truth mask\n",
    "                    pred = cv2.resize(pred[0], (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                ground_truth_binary = gt == 255\n",
    "                predicted_binary = pred == 255\n",
    "                iou = compute_iou(ground_truth_binary, predicted_binary)\n",
    "                precision, recall, f1 = compute_f1(ground_truth_binary, predicted_binary)\n",
    "                iou_sum += iou\n",
    "                f1_sum += f1\n",
    "                recall_sum += recall\n",
    "                precision_sum += precision\n",
    "                count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                mean_iou = iou_sum / count\n",
    "                mean_f1 = f1_sum / count\n",
    "                mean_recall = recall_sum / count\n",
    "                mean_precision = precision_sum / count\n",
    "                results[dataset][prompt_type].append((perturbation, mean_iou, mean_f1, mean_recall, mean_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easily_influenced_results = []\n",
    "difficult_influenced_results = []\n",
    "\n",
    "for dataset, data_label_prefix in zip(datasets, data_label_prefixes):\n",
    "    for prompt_type in prompt_types:\n",
    "        min_iou = float('inf')\n",
    "        max_iou = float('-inf')\n",
    "        easily_influenced = None\n",
    "        difficult_to_influence = None\n",
    "\n",
    "        for perturbation in perturbations:\n",
    "            iou_sum = 0\n",
    "            count = 0\n",
    "            for i in range(1, 11):\n",
    "                data_label = f'{data_label_prefix}{i}'\n",
    "                gt_path = os.path.join(str(dataset), f'{data_label}_raw_gt.png' if dataset in ('ultrasound', 'radiology', 'fish', 'dancing', 'road') else f'{data_label}_raw_gt.jpg')\n",
    "                pred_path = os.path.join(dataset, f'{data_label}_{perturbation}_{prompt_type}_mask.npy')\n",
    "                gt = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "                pred = np.load(pred_path)\n",
    "\n",
    "                if data_label_prefix == \"r\":\n",
    "                    # Resize the predicted masks to match the size of the ground truth mask\n",
    "                    pred = cv2.resize(pred[0], (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                ground_truth_binary = gt == 255\n",
    "                predicted_binary = pred == 255\n",
    "                iou = compute_iou(ground_truth_binary, predicted_binary)\n",
    "                iou_sum += iou\n",
    "                count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                mean_iou = iou_sum / count\n",
    "                if mean_iou < min_iou:\n",
    "                    min_iou = mean_iou\n",
    "                    easily_influenced = perturbation\n",
    "\n",
    "                if mean_iou > max_iou:\n",
    "                    max_iou = mean_iou\n",
    "                    difficult_to_influence = perturbation\n",
    "\n",
    "        easily_influenced_results.append((dataset, prompt_type, easily_influenced, min_iou))\n",
    "        difficult_influenced_results.append((dataset, prompt_type, difficult_to_influence, max_iou))\n",
    "\n",
    "# Print the easily influenced results\n",
    "print(\"Dataset\\tPrompt Type\\tEasily Influenced\\tIoU\")\n",
    "for res in easily_influenced_results:\n",
    "    print(f\"{res[0]}\\t{res[1]}\\t{res[2]}\\t{res[3]:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the difficult to influence results\n",
    "print(\"Dataset\\tPrompt Type\\tDifficult to Influence\\tIoU\")\n",
    "for res in difficult_influenced_results:\n",
    "    print(f\"{res[0]}\\t{res[1]}\\t{res[2]}\\t{res[3]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence perturbations\n",
    "final_results = {}\n",
    "for dataset in datasets:\n",
    "    final_results[dataset] = {}\n",
    "    for prompt_type in prompt_types:\n",
    "        sorted_results = sorted(results[dataset][prompt_type], key=lambda x: x[1])\n",
    "        easily_influenced = sorted_results[:top_k]\n",
    "        difficult_to_influence = sorted_results[-top_k:][::-1]\n",
    "        final_results[dataset][prompt_type] = {\"easy\": easily_influenced, \"difficult\": difficult_to_influence}\n",
    "\n",
    "# Print the results in a tabular format\n",
    "print(\"Dataset\\tPrompt Type\\tEasily Influenced (Top k)\\tDifficult to Influence (Top k)\")\n",
    "for dataset in final_results:\n",
    "    for prompt_type in final_results[dataset]:\n",
    "        easy = \", \".join([f\"{item[0]} ({item[1]:.4f})\" for item in final_results[dataset][prompt_type][\"easy\"]])\n",
    "        difficult = \", \".join([f\"{item[0]} ({item[1]:.4f})\" for item in final_results[dataset][prompt_type][\"difficult\"]])\n",
    "        print(f\"{dataset}\\t{prompt_type}\\t{easy}\\t{difficult}\")\n",
    "\n",
    "print(\"\\nDataset\\tPrompt Type\\tPerturbation\\tIoU\\tF1\\tRecall\\tPrecision\")\n",
    "for dataset in results:\n",
    "    for prompt_type in results[dataset]:\n",
    "        for res in results[dataset][prompt_type]:\n",
    "            print(f\"{dataset}\\t{prompt_type}\\t{res[0]}\\t{res[1]:.4f}\\t{res[2]:.4f}\\t{res[3]:.4f}\\t{res[4]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for top-k easily influenced and difficult to influence perturbations\n",
    "influence_results = []\n",
    "for dataset in final_results:\n",
    "    best_prompt = None\n",
    "    best_prompt_score = float('-inf')\n",
    "    top_k_easy = []\n",
    "    top_k_difficult = []\n",
    "\n",
    "    for prompt_type in final_results[dataset]:\n",
    "        easy_sorted = sorted(final_results[dataset][prompt_type][\"easy\"], key=lambda x: x[1])\n",
    "        difficult_sorted = sorted(final_results[dataset][prompt_type][\"difficult\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        easy_score = sum([item[1] for item in easy_sorted[:4]])\n",
    "        difficult_score = sum([item[1] for item in difficult_sorted[:4]])\n",
    "        total_score = easy_score + difficult_score\n",
    "\n",
    "        if total_score > best_prompt_score:\n",
    "            best_prompt_score = total_score\n",
    "            best_prompt = prompt_type\n",
    "            top_k_easy = easy_sorted[:4]\n",
    "            top_k_difficult = difficult_sorted[:4]\n",
    "\n",
    "    easy_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_easy])\n",
    "    difficult_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_difficult])\n",
    "\n",
    "    influence_results.append([dataset, best_prompt, easy_perturbations, difficult_perturbations])\n",
    "\n",
    "iou_influence_df = pd.DataFrame(influence_results, columns=[\"Dataset\", \"Most Suitable Prompt\", \"Top-4 Easily Influenced\", \"Top-4 Difficult to Influence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for top-k easily influenced and difficult to influence perturbations\n",
    "influence_results = []\n",
    "for dataset in final_results:\n",
    "    best_prompt = None\n",
    "    best_prompt_score = float('-inf')\n",
    "    top_k_easy = []\n",
    "    top_k_difficult = []\n",
    "\n",
    "    for prompt_type in final_results[dataset]:\n",
    "        easy_sorted = sorted(final_results[dataset][prompt_type][\"easy\"], key=lambda x: x[1])\n",
    "        difficult_sorted = sorted(final_results[dataset][prompt_type][\"difficult\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        easy_score = sum([item[2] for item in easy_sorted[:4]])\n",
    "        difficult_score = sum([item[2] for item in difficult_sorted[:4]])\n",
    "        total_score = easy_score + difficult_score\n",
    "\n",
    "        if total_score > best_prompt_score:\n",
    "            best_prompt_score = total_score\n",
    "            best_prompt = prompt_type\n",
    "            top_k_easy = easy_sorted[:4]\n",
    "            top_k_difficult = difficult_sorted[:4]\n",
    "\n",
    "    easy_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_easy])\n",
    "    difficult_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_difficult])\n",
    "\n",
    "    influence_results.append([dataset, best_prompt, easy_perturbations, difficult_perturbations])\n",
    "\n",
    "f1_influence_df = pd.DataFrame(influence_results, columns=[\"Dataset\", \"Most Suitable Prompt\", \"Top-4 Easily Influenced\", \"Top-4 Difficult to Influence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for top-k easily influenced and difficult to influence perturbations\n",
    "influence_results = []\n",
    "for dataset in final_results:\n",
    "    best_prompt = None\n",
    "    best_prompt_score = float('-inf')\n",
    "    top_k_easy = []\n",
    "    top_k_difficult = []\n",
    "\n",
    "    for prompt_type in final_results[dataset]:\n",
    "        easy_sorted = sorted(final_results[dataset][prompt_type][\"easy\"], key=lambda x: x[1])\n",
    "        difficult_sorted = sorted(final_results[dataset][prompt_type][\"difficult\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        easy_score = sum([item[3] for item in easy_sorted[:4]])\n",
    "        difficult_score = sum([item[3] for item in difficult_sorted[:4]])\n",
    "        total_score = easy_score + difficult_score\n",
    "\n",
    "        if total_score > best_prompt_score:\n",
    "            best_prompt_score = total_score\n",
    "            best_prompt = prompt_type\n",
    "            top_k_easy = easy_sorted[:4]\n",
    "            top_k_difficult = difficult_sorted[:4]\n",
    "\n",
    "    easy_perturbations = \", \".join([f\"{item[0]} ({item[3]:.3f})\" for item in top_k_easy])\n",
    "    difficult_perturbations = \", \".join([f\"{item[0]} ({item[3]:.3f})\" for item in top_k_difficult])\n",
    "\n",
    "    influence_results.append([dataset, best_prompt, easy_perturbations, difficult_perturbations])\n",
    "\n",
    "recall_influence_df = pd.DataFrame(influence_results, columns=[\"Dataset\", \"Most Suitable Prompt\", \"Top-4 Easily Influenced\", \"Top-4 Difficult to Influence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for top-k easily influenced and difficult to influence perturbations\n",
    "influence_results = []\n",
    "for dataset in final_results:\n",
    "    best_prompt = None\n",
    "    best_prompt_score = float('-inf')\n",
    "    top_k_easy = []\n",
    "    top_k_difficult = []\n",
    "\n",
    "    for prompt_type in final_results[dataset]:\n",
    "        easy_sorted = sorted(final_results[dataset][prompt_type][\"easy\"], key=lambda x: x[1])\n",
    "        difficult_sorted = sorted(final_results[dataset][prompt_type][\"difficult\"], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        easy_score = sum([item[4] for item in easy_sorted[:4]])\n",
    "        difficult_score = sum([item[4] for item in difficult_sorted[:4]])\n",
    "        total_score = easy_score + difficult_score\n",
    "\n",
    "        if total_score > best_prompt_score:\n",
    "            best_prompt_score = total_score\n",
    "            best_prompt = prompt_type\n",
    "            top_k_easy = easy_sorted[:4]\n",
    "            top_k_difficult = difficult_sorted[:4]\n",
    "\n",
    "    easy_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_easy])\n",
    "    difficult_perturbations = \", \".join([f\"{item[0]}\" for item in top_k_difficult])\n",
    "\n",
    "    influence_results.append([dataset, best_prompt, easy_perturbations, difficult_perturbations])\n",
    "\n",
    "precision_influence_df = pd.DataFrame(influence_results, columns=[\"Dataset\", \"Most Suitable Prompt\", \"Top-4 Easily Influenced\", \"Top-4 Difficult to Influence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def majority_vote(top3_lists):\n",
    "    counter = {}\n",
    "    for top3 in top3_lists:\n",
    "        for item in top3.split(', '):\n",
    "            perturbation_name = re.match(r\"([\\w_]+)\", item).group(1)\n",
    "            if perturbation_name not in counter:\n",
    "                counter[perturbation_name] = 0\n",
    "            counter[perturbation_name] += 1\n",
    "    sorted_items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_items = [item[0] for item in sorted_items[:4]]\n",
    "    return \", \".join(top_items)\n",
    "\n",
    "# Get the lists of top-3 easily influenced and difficult to influence perturbations\n",
    "iou_easy = iou_influence_df[\"Top-4 Easily Influenced\"].tolist()\n",
    "f1_easy = f1_influence_df[\"Top-4 Easily Influenced\"].tolist()\n",
    "recall_easy = recall_influence_df[\"Top-4 Easily Influenced\"].tolist()\n",
    "precision_easy = precision_influence_df[\"Top-4 Easily Influenced\"].tolist()\n",
    "\n",
    "iou_difficult = iou_influence_df[\"Top-4 Difficult to Influence\"].tolist()\n",
    "f1_difficult = f1_influence_df[\"Top-4 Difficult to Influence\"].tolist()\n",
    "recall_difficult = recall_influence_df[\"Top-4 Difficult to Influence\"].tolist()\n",
    "precision_difficult = precision_influence_df[\"Top-4 Difficult to Influence\"].tolist()\n",
    "\n",
    "# Create the final dataframe using majority voting\n",
    "final_data = []\n",
    "for idx, dataset in enumerate(iou_influence_df[\"Dataset\"]):\n",
    "    most_suitable_prompt = majority_vote([iou_influence_df.at[idx, \"Most Suitable Prompt\"],\n",
    "                                          f1_influence_df.at[idx, \"Most Suitable Prompt\"],\n",
    "                                          recall_influence_df.at[idx, \"Most Suitable Prompt\"],\n",
    "                                          precision_influence_df.at[idx, \"Most Suitable Prompt\"]])\n",
    "    \n",
    "    easy = majority_vote([iou_easy[idx], f1_easy[idx], recall_easy[idx], precision_easy[idx]])\n",
    "    difficult = majority_vote([iou_difficult[idx], f1_difficult[idx], recall_difficult[idx], precision_difficult[idx]])\n",
    "    \n",
    "    final_data.append([dataset, most_suitable_prompt, easy, difficult])\n",
    "\n",
    "final_df = pd.DataFrame(final_data, columns=[\"Dataset\", \"Most Suitable Prompt (Majority Vote)\", \"Top-4 Easily Influenced (Majority Vote)\", \"Top-4 Difficult to Influence (Majority Vote)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
